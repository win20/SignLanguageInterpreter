{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7883ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
    "import scipy.stats as stats\n",
    "from sklearn import preprocessing, model_selection\n",
    "\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3303aebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frames(df, frame_size, hop_size):\n",
    "    N_FEATURES = 140    \n",
    "    \n",
    "    frames = []\n",
    "    labels = []  \n",
    "    for i in range(0, len(df) - frame_size, hop_size):\n",
    "        hands = df['hands'].values[i: i + frame_size]\n",
    "        fingers = df['fingers'].values[i: i + frame_size]\n",
    "        lh_palm_pos_x = df['lh_palm_pos_x'].values[i: i + frame_size]\n",
    "        lh_palm_pos_y = df['lh_palm_pos_y'].values[i: i + frame_size]\n",
    "        lh_palm_pos_z = df['lh_palm_pos_z'].values[i: i + frame_size]      \n",
    "        rh_palm_pos_x = df['rh_palm_pos_x'].values[i: i + frame_size]\n",
    "        rh_palm_pos_y = df['rh_palm_pos_y'].values[i: i + frame_size]\n",
    "        rh_palm_pos_z = df['rh_palm_pos_z'].values[i: i + frame_size]\n",
    "        lh_normal_x = df['lh_normal_x'].values[i: i + frame_size]\n",
    "        lh_normal_y = df['lh_normal_y'].values[i: i + frame_size]\n",
    "        lh_normal_z = df['lh_normal_z'].values[i: i + frame_size]\n",
    "        rh_normal_x = df['rh_normal_x'].values[i: i + frame_size]\n",
    "        rh_normal_y = df['rh_normal_y'].values[i: i + frame_size]\n",
    "        rh_normal_z = df['rh_normal_z'].values[i: i + frame_size]\n",
    "        lh_direction_x = df['lh_direction_x'].values[i: i + frame_size]\n",
    "        lh_direction_y = df['lh_direction_y'].values[i: i + frame_size]\n",
    "        lh_direction_z = df['lh_direction_z'].values[i: i + frame_size]\n",
    "        rh_direction_x = df['rh_direction_x'].values[i: i + frame_size]\n",
    "        rh_direction_y = df['rh_direction_y'].values[i: i + frame_size]\n",
    "        rh_direction_z = df['rh_direction_z'].values[i: i + frame_size]\n",
    "        lh_index_direction_x = df['lh_index_direction_x'].values[i: i + frame_size]\n",
    "        lh_index_direction_y = df['lh_index_direction_y'].values[i: i + frame_size]\n",
    "        lh_index_direction_z = df['lh_index_direction_z'].values[i: i + frame_size]\n",
    "        lh_index_extended = df['lh_index_extended'].values[i: i + frame_size]\n",
    "        lh_index_tip_x = df['lh_index_tip_x'].values[i: i + frame_size]\n",
    "        lh_index_tip_y = df['lh_index_tip_y'].values[i: i + frame_size]\n",
    "        lh_index_tip_z = df['lh_index_tip_z'].values[i: i + frame_size]\n",
    "        lh_index_velocity_x = df['lh_index_velocity_x'].values[i: i + frame_size]\n",
    "        lh_index_velocity_y = df['lh_index_velocity_y'].values[i: i + frame_size]\n",
    "        lh_index_velocity_z = df['lh_index_velocity_z'].values[i: i + frame_size]\n",
    "        lh_middle_direction_x = df['lh_middle_direction_x'].values[i: i + frame_size]\n",
    "        lh_middle_direction_y = df['lh_middle_direction_y'].values[i: i + frame_size]\n",
    "        lh_middle_direction_z = df['lh_middle_direction_z'].values[i: i + frame_size]\n",
    "        lh_middle_extended = df['lh_middle_extended'].values[i: i + frame_size]\n",
    "        lh_middle_tip_x = df['lh_middle_tip_x'].values[i: i + frame_size]\n",
    "        lh_middle_tip_y = df['lh_middle_tip_y'].values[i: i + frame_size]\n",
    "        lh_middle_tip_z = df['lh_middle_tip_z'].values[i: i + frame_size]\n",
    "        lh_middle_velocity_x = df['lh_middle_velocity_x'].values[i: i + frame_size]\n",
    "        lh_middle_velocity_y = df['lh_middle_velocity_y'].values[i: i + frame_size]\n",
    "        lh_middle_velocity_z = df['lh_middle_velocity_z'].values[i: i + frame_size]\n",
    "        lh_palm_vel_x = df['lh_palm_vel_x'].values[i: i + frame_size]       \n",
    "        lh_palm_vel_y = df['lh_palm_vel_y'].values[i: i + frame_size]\n",
    "        lh_palm_vel_z = df['lh_palm_vel_z'].values[i: i + frame_size]\n",
    "        lh_pinky_direction_x = df['lh_pinky_direction_x'].values[i: i + frame_size]\n",
    "        lh_pinky_direction_y = df['lh_pinky_direction_y'].values[i: i + frame_size]\n",
    "        lh_pinky_direction_z = df['lh_pinky_direction_z'].values[i: i + frame_size]\n",
    "        lh_pinky_extended = df['lh_pinky_extended'].values[i: i + frame_size]\n",
    "        lh_pinky_tip_x = df['lh_pinky_tip_x'].values[i: i + frame_size]\n",
    "        lh_pinky_tip_y = df['lh_pinky_tip_y'].values[i: i + frame_size]\n",
    "        lh_pinky_tip_z = df['lh_pinky_tip_z'].values[i: i + frame_size]\n",
    "        lh_pinky_velocity_x = df['lh_pinky_velocity_x'].values[i: i + frame_size]\n",
    "        lh_pinky_velocity_y = df['lh_pinky_velocity_y'].values[i: i + frame_size]\n",
    "        lh_pinky_velocity_z = df['lh_pinky_velocity_z'].values[i: i + frame_size]\n",
    "        lh_ring_direction_x = df['lh_ring_direction_x'].values[i: i + frame_size]\n",
    "        lh_ring_direction_y = df['lh_ring_direction_y'].values[i: i + frame_size]\n",
    "        lh_ring_direction_z = df['lh_ring_direction_z'].values[i: i + frame_size]\n",
    "        lh_ring_extended = df['lh_ring_extended'].values[i: i + frame_size]\n",
    "        lh_ring_tip_x = df['lh_ring_tip_x'].values[i: i + frame_size]\n",
    "        lh_ring_tip_y = df['lh_ring_tip_y'].values[i: i + frame_size]\n",
    "        lh_ring_tip_z = df['lh_ring_tip_z'].values[i: i + frame_size]\n",
    "        lh_ring_velocity_x = df['lh_ring_velocity_x'].values[i: i + frame_size]\n",
    "        lh_ring_velocity_y = df['lh_ring_velocity_y'].values[i: i + frame_size]\n",
    "        lh_ring_velocity_z = df['lh_ring_velocity_z'].values[i: i + frame_size]\n",
    "        lh_sphere_center_x = df['lh_sphere_center_x'].values[i: i + frame_size]\n",
    "        lh_sphere_center_y = df['lh_sphere_center_y'].values[i: i + frame_size]\n",
    "        lh_sphere_center_z = df['lh_sphere_center_z'].values[i: i + frame_size]\n",
    "        lh_sphere_radius = df['lh_sphere_radius'].values[i: i + frame_size]\n",
    "        lh_thumb_direction_x = df['lh_thumb_direction_x'].values[i: i + frame_size]\n",
    "        lh_thumb_direction_y = df['lh_thumb_direction_y'].values[i: i + frame_size]\n",
    "        lh_thumb_direction_z = df['lh_thumb_direction_z'].values[i: i + frame_size]\n",
    "        lh_thumb_extended = df['lh_thumb_extended'].values[i: i + frame_size]        \n",
    "        lh_thumb_tip_x = df['lh_thumb_tip_x'].values[i: i + frame_size]\n",
    "        lh_thumb_tip_y = df['lh_thumb_tip_y'].values[i: i + frame_size]\n",
    "        lh_thumb_tip_z = df['lh_thumb_tip_z'].values[i: i + frame_size]\n",
    "        lh_thumb_velocity_x = df['lh_thumb_velocity_x'].values[i: i + frame_size]\n",
    "        lh_thumb_velocity_y = df['lh_thumb_velocity_y'].values[i: i + frame_size]\n",
    "        lh_thumb_velocity_z = df['lh_thumb_velocity_z'].values[i: i + frame_size]\n",
    "        lh_wrist_pos_x = df['lh_wrist_pos_x'].values[i: i + frame_size]\n",
    "        lh_wrist_pos_y = df['lh_wrist_pos_y'].values[i: i + frame_size]\n",
    "        lh_wrist_pos_z = df['lh_wrist_pos_z'].values[i: i + frame_size]\n",
    "        rh_index_direction_x = df['rh_index_direction_x'].values[i: i + frame_size]\n",
    "        rh_index_direction_y = df['rh_index_direction_y'].values[i: i + frame_size]\n",
    "        rh_index_direction_z = df['rh_index_direction_z'].values[i: i + frame_size]\n",
    "        rh_index_extended = df['rh_index_extended'].values[i: i + frame_size]\n",
    "        rh_index_tip_x = df['rh_index_tip_x'].values[i: i + frame_size]\n",
    "        rh_index_tip_y = df['rh_index_tip_y'].values[i: i + frame_size]\n",
    "        rh_index_tip_z = df['rh_index_tip_z'].values[i: i + frame_size]\n",
    "        rh_index_velocity_x = df['rh_index_velocity_x'].values[i: i + frame_size]\n",
    "        rh_index_velocity_y = df['rh_index_velocity_y'].values[i: i + frame_size]\n",
    "        rh_index_velocity_z = df['rh_index_velocity_z'].values[i: i + frame_size]\n",
    "        rh_middle_direction_x = df['rh_middle_direction_x'].values[i: i + frame_size]\n",
    "        rh_middle_direction_y = df['rh_middle_direction_y'].values[i: i + frame_size]\n",
    "        rh_middle_direction_z = df['rh_middle_direction_z'].values[i: i + frame_size]\n",
    "        rh_middle_extended = df['rh_middle_extended'].values[i: i + frame_size]\n",
    "        rh_middle_tip_x = df['rh_middle_tip_x'].values[i: i + frame_size]\n",
    "        rh_middle_tip_y = df['rh_middle_tip_y'].values[i: i + frame_size]\n",
    "        rh_middle_tip_z = df['rh_middle_tip_z'].values[i: i + frame_size]\n",
    "        rh_middle_velocity_x = df['rh_middle_velocity_x'].values[i: i + frame_size]\n",
    "        rh_middle_velocity_y = df['rh_middle_velocity_y'].values[i: i + frame_size]\n",
    "        rh_middle_velocity_z = df['rh_middle_velocity_z'].values[i: i + frame_size]\n",
    "        rh_palm_vel_x = df['rh_palm_vel_x'].values[i: i + frame_size]\n",
    "        rh_palm_vel_y = df['rh_palm_vel_y'].values[i: i + frame_size]\n",
    "        rh_palm_vel_z = df['rh_palm_vel_z'].values[i: i + frame_size]\n",
    "        rh_pinky_direction_x = df['rh_pinky_direction_x'].values[i: i + frame_size]\n",
    "        rh_pinky_direction_y = df['rh_pinky_direction_y'].values[i: i + frame_size]\n",
    "        rh_pinky_direction_z = df['rh_pinky_direction_z'].values[i: i + frame_size]\n",
    "        rh_pinky_extended = df['rh_pinky_extended'].values[i: i + frame_size]\n",
    "        rh_pinky_tip_x = df['rh_pinky_tip_x'].values[i: i + frame_size]\n",
    "        rh_pinky_tip_y = df['rh_pinky_tip_y'].values[i: i + frame_size]\n",
    "        rh_pinky_tip_z = df['rh_pinky_tip_z'].values[i: i + frame_size]        \n",
    "        rh_pinky_velocity_x = df['rh_pinky_velocity_x'].values[i: i + frame_size]\n",
    "        rh_pinky_velocity_y = df['rh_pinky_velocity_y'].values[i: i + frame_size]\n",
    "        rh_pinky_velocity_z = df['rh_pinky_velocity_z'].values[i: i + frame_size]\n",
    "        rh_ring_direction_x = df['rh_ring_direction_x'].values[i: i + frame_size]\n",
    "        rh_ring_direction_y = df['rh_ring_direction_y'].values[i: i + frame_size]\n",
    "        rh_ring_direction_z = df['rh_ring_direction_z'].values[i: i + frame_size]\n",
    "        rh_ring_extended = df['rh_ring_extended'].values[i: i + frame_size]\n",
    "        rh_ring_tip_x = df['rh_ring_tip_x'].values[i: i + frame_size]\n",
    "        rh_ring_tip_y = df['rh_ring_tip_y'].values[i: i + frame_size]\n",
    "        rh_ring_tip_z = df['rh_ring_tip_z'].values[i: i + frame_size]\n",
    "        rh_ring_velocity_x = df['rh_ring_velocity_x'].values[i: i + frame_size]\n",
    "        rh_ring_velocity_y = df['rh_ring_velocity_y'].values[i: i + frame_size]\n",
    "        rh_ring_velocity_z = df['rh_ring_velocity_z'].values[i: i + frame_size]\n",
    "        rh_sphere_center_x = df['rh_sphere_center_x'].values[i: i + frame_size]\n",
    "        rh_sphere_center_y = df['rh_sphere_center_y'].values[i: i + frame_size]\n",
    "        rh_sphere_center_z = df['rh_sphere_center_z'].values[i: i + frame_size]\n",
    "        rh_sphere_radius = df['rh_sphere_radius'].values[i: i + frame_size]\n",
    "        rh_thumb_direction_x = df['rh_thumb_direction_x'].values[i: i + frame_size]\n",
    "        rh_thumb_direction_y = df['rh_thumb_direction_y'].values[i: i + frame_size]\n",
    "        rh_thumb_direction_z = df['rh_thumb_direction_z'].values[i: i + frame_size]\n",
    "        rh_thumb_extended = df['rh_thumb_extended'].values[i: i + frame_size]\n",
    "        rh_thumb_tip_x = df['rh_thumb_tip_x'].values[i: i + frame_size]\n",
    "        rh_thumb_tip_y = df['rh_thumb_tip_y'].values[i: i + frame_size]\n",
    "        rh_thumb_tip_z = df['rh_thumb_tip_z'].values[i: i + frame_size]\n",
    "        rh_thumb_velocity_x = df['rh_thumb_velocity_x'].values[i: i + frame_size]\n",
    "        rh_thumb_velocity_y = df['rh_thumb_velocity_y'].values[i: i + frame_size]\n",
    "        rh_thumb_velocity_z = df['rh_thumb_velocity_z'].values[i: i + frame_size]   \n",
    "        rh_wrist_pos_x = df['rh_wrist_pos_x'].values[i: i + frame_size]\n",
    "        rh_wrist_pos_y = df['rh_wrist_pos_y'].values[i: i + frame_size]\n",
    "        rh_wrist_pos_z = df['rh_wrist_pos_z'].values[i: i + frame_size]\n",
    "        \n",
    "        # Retrieve the most common label in this data sample\n",
    "        label = stats.mode(df['label'][i: i + frame_size])[0][0]\n",
    "        frames.append([hands,fingers,lh_palm_pos_x,lh_palm_pos_y,lh_palm_pos_z,rh_palm_pos_x,rh_palm_pos_y,rh_palm_pos_z,lh_normal_x,lh_normal_y,lh_normal_z,rh_normal_x,rh_normal_y,rh_normal_z,lh_direction_x,lh_direction_y,lh_direction_z,rh_direction_x,rh_direction_y,rh_direction_z,lh_index_direction_x,lh_index_direction_y,lh_index_direction_z,lh_index_extended,lh_index_tip_x,lh_index_tip_y,lh_index_tip_z,lh_index_velocity_x,lh_index_velocity_y,lh_index_velocity_z,lh_middle_direction_x,lh_middle_direction_y,lh_middle_direction_z,lh_middle_extended,lh_middle_tip_x,lh_middle_tip_y,lh_middle_tip_z,lh_middle_velocity_x,lh_middle_velocity_y,lh_middle_velocity_z,lh_palm_vel_x,lh_palm_vel_y,lh_palm_vel_z,lh_pinky_direction_x,lh_pinky_direction_y,lh_pinky_direction_z,lh_pinky_extended,lh_pinky_tip_x,lh_pinky_tip_y,lh_pinky_tip_z,lh_pinky_velocity_x,lh_pinky_velocity_y,lh_pinky_velocity_z,lh_ring_direction_x,lh_ring_direction_y,lh_ring_direction_z,lh_ring_extended,lh_ring_tip_x,lh_ring_tip_y,lh_ring_tip_z,lh_ring_velocity_x,lh_ring_velocity_y,lh_ring_velocity_z,lh_sphere_center_x,lh_sphere_center_y,lh_sphere_center_z,lh_sphere_radius,lh_thumb_direction_x,lh_thumb_direction_y,lh_thumb_direction_z,lh_thumb_extended,lh_thumb_tip_x,lh_thumb_tip_y,lh_thumb_tip_z,lh_thumb_velocity_x,lh_thumb_velocity_y,lh_thumb_velocity_z,lh_wrist_pos_x,lh_wrist_pos_y,lh_wrist_pos_z,rh_index_direction_x,rh_index_direction_y,rh_index_direction_z,rh_index_extended,rh_index_tip_x,rh_index_tip_y,rh_index_tip_z,rh_index_velocity_x,rh_index_velocity_y,rh_index_velocity_z,rh_middle_direction_x,rh_middle_direction_y,rh_middle_direction_z,rh_middle_extended,rh_middle_tip_x,rh_middle_tip_y,rh_middle_tip_z,rh_middle_velocity_x,rh_middle_velocity_y,rh_middle_velocity_z,rh_palm_vel_x,rh_palm_vel_y,rh_palm_vel_z,rh_pinky_direction_x,rh_pinky_direction_y,rh_pinky_direction_z,rh_pinky_extended,rh_pinky_tip_x,rh_pinky_tip_y,rh_pinky_tip_z,rh_pinky_velocity_x,rh_pinky_velocity_y,rh_pinky_velocity_z,rh_ring_direction_x,rh_ring_direction_y,rh_ring_direction_z,rh_ring_extended,rh_ring_tip_x,rh_ring_tip_y,rh_ring_tip_z,rh_ring_velocity_x,rh_ring_velocity_y,rh_ring_velocity_z,rh_sphere_center_x,rh_sphere_center_y,rh_sphere_center_z,rh_sphere_radius,rh_thumb_direction_x,rh_thumb_direction_y,rh_thumb_direction_z,rh_thumb_extended,rh_thumb_tip_x,rh_thumb_tip_y,rh_thumb_tip_z,rh_thumb_velocity_x,rh_thumb_velocity_y,rh_thumb_velocity_z,rh_wrist_pos_x,rh_wrist_pos_y,rh_wrist_pos_z])\n",
    "        labels.append(label)\n",
    "            \n",
    "     # Convert to np array\n",
    "    frames = np.asarray(frames).reshape(-1, frame_size, N_FEATURES)\n",
    "    labels = np.asarray(labels)\n",
    "    \n",
    "    return frames, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d05f2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling_prediction(data_path):\n",
    "    data = pd.read_csv(data_path)\n",
    "    expected_label = data['label']\n",
    "    label = LabelEncoder()\n",
    "    data['label'] = label.fit_transform(data['label'])\n",
    "    \n",
    "    Fs = 10\n",
    "    frame_size = Fs * 2       # 20 samples\n",
    "    hop_size = Fs * 1         # increment of 10 samples\n",
    "    \n",
    "    X, y = get_frames(data, frame_size, hop_size)\n",
    "    X = X.reshape(X.shape[0], X.shape[1], X.shape[2], 1)\n",
    "    \n",
    "    # load and process model\n",
    "    model = tf.keras.models.load_model('saved_models/sampling/')\n",
    "    probability_model = tf.keras.Sequential([model, tf.keras.layers.Softmax()])\n",
    "\n",
    "    # make predictions on data and find most common prediction\n",
    "    predictions = probability_model.predict(X)\n",
    "    prediction_indices = []\n",
    "    for item in predictions:\n",
    "        prediction_indices.append(np.argmax(item))\n",
    "\n",
    "    counter = 0\n",
    "    max_num = prediction_indices[0]\n",
    "    for item in prediction_indices:\n",
    "        current_frequency = prediction_indices.count(item)\n",
    "        if current_frequency > counter:\n",
    "            counter = current_frequency\n",
    "            max_num = item\n",
    "\n",
    "    labels = ['eight', 'five', 'four', 'nine', 'one', 'seven', 'six', 'ten', 'three', 'two', 'zero']\n",
    "    prediction = labels[max_num]\n",
    "    \n",
    "    return prediction, expected_label[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b79d170",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(path):\n",
    "    filelist = glob.glob(os.path.join(path, '*.csv'))\n",
    "    \n",
    "    # list of files that should be at the end,\n",
    "    # needed because of the order glob gets the files\n",
    "    tail = ['data\\\\new_data\\\\10-0.csv', \n",
    "            'data\\\\new_data\\\\10-1.csv', \n",
    "            'data\\\\new_data\\\\10-2.csv', \n",
    "            'data\\\\new_data\\\\10-3.csv', \n",
    "            'data\\\\new_data\\\\10-4.csv',\n",
    "            'data\\\\new_data\\\\10-5.csv', \n",
    "            'data\\\\new_data\\\\10-6.csv', \n",
    "            'data\\\\new_data\\\\10-7.csv', \n",
    "            'data\\\\new_data\\\\10-8.csv', \n",
    "            'data\\\\new_data\\\\10-9.csv']\n",
    "    \n",
    "    # add the files except those in tail\n",
    "    files = []\n",
    "    for file in filelist:\n",
    "        if file not in tail:\n",
    "            files.append(file)\n",
    "\n",
    "    files = files + tail\n",
    "    \n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4bcd301",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(data_path, model):\n",
    "    data = pd.read_csv(data_path)\n",
    "    \n",
    "    dataset = data.values\n",
    "    X = dataset[:,0:140].astype(float)\n",
    "    expected_label = data['label']\n",
    "    \n",
    "    # load and process model\n",
    "    model = tf.keras.models.load_model(model)\n",
    "    probability_model = tf.keras.Sequential([model, tf.keras.layers.Softmax()])\n",
    "\n",
    "    # make predictions on data and find most common prediction\n",
    "    predictions = probability_model.predict(X)\n",
    "    prediction_indices = []\n",
    "    for item in predictions:\n",
    "        prediction_indices.append(np.argmax(item))\n",
    "\n",
    "    counter = 0\n",
    "    max_num = prediction_indices[0]\n",
    "    for item in prediction_indices:\n",
    "        current_frequency = prediction_indices.count(item)\n",
    "        if current_frequency > counter:\n",
    "            counter = current_frequency\n",
    "            max_num = item\n",
    "\n",
    "    labels = ['eight', 'five', 'four', 'nine', 'one', 'seven', 'six', 'ten', 'three', 'two', 'zero']\n",
    "    \n",
    "    prediction = labels[max_num]\n",
    "    \n",
    "    return prediction, expected_label[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c33cdd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_all_files(files_list, model):\n",
    "    predictions_list = []\n",
    "    \n",
    "    for file in files_list:\n",
    "        if model == 'saved_models/sampling/':\n",
    "            predictions_list.append(sampling_prediction(file))\n",
    "        else:\n",
    "            predictions_list.append(make_prediction(file, model))\n",
    "        \n",
    "    return predictions_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86a12a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(pred_list, class_rankings, out_path):\n",
    "    with open(out_path, 'w') as f:\n",
    "        f.write('===============================================\\n')\n",
    "        f.write('                MODEL TESTING\\n')\n",
    "        f.write('===============================================\\n\\n\\n')\n",
    "        \n",
    "        perc_right, perc_wrong = calculate_accuracy(pred_list)\n",
    "        f.write('Overall model accuracy\\n==============\\n')\n",
    "        f.write('correct: %.2f %%\\n' % perc_right)\n",
    "        f.write('incorrect: %.2f %%\\n\\n' % perc_wrong)\n",
    "        \n",
    "        f.write('Predicted labels\\n==============\\n')\n",
    "        for item in pred_list:\n",
    "            f.write('Predicted class: ' + item[0] + ' - True class: ' + item[1] + '\\n')\n",
    "            \n",
    "        f.write('\\nPer class rankings\\n==============\\n')\n",
    "        for key, value in class_rankings.items():\n",
    "            f.write('Class: %s - correct: %.2f %% - incorrect: %.2f %%\\n' % (key, value[0], value[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9849060d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(pred_list):\n",
    "    different_count = 0\n",
    "    for item in pred_list:\n",
    "        if item[0] != item[1]:\n",
    "            different_count += 1\n",
    "    \n",
    "    percentage_wrong = (different_count / len(pred_list)) * 100\n",
    "    percentage_right = 100 - percentage_wrong\n",
    "    \n",
    "    return percentage_right, percentage_wrong\n",
    "\n",
    "\n",
    "def rank_class_performance(pred_list):\n",
    "    predictions = np.array([*pred_list])\n",
    "    predictions = np.reshape(predictions, (11, 10, 2)) \n",
    "\n",
    "    per_class_accuracy = {}\n",
    "    for group in predictions:\n",
    "        for item in group:\n",
    "            perc_right, perc_wrong = calculate_accuracy(group)\n",
    "            per_class_accuracy.update({item[1]: [perc_right, perc_wrong]})\n",
    "    \n",
    "    sorted_dict = {k: v for k, v in sorted(per_class_accuracy.items(), key=lambda item: item[1])}\n",
    "    return sorted_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f998af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_test(model_type):\n",
    "    if model_type == 0:\n",
    "        predictions_list = predict_all_files(files, 'saved_models/no_scaling/')\n",
    "        per_class_dict = rank_class_performance(predictions_list)\n",
    "        write_to_file(predictions_list, per_class_dict, 'model_testing/new_data/no_scaling_test.txt')\n",
    "    elif model_type == 1:\n",
    "        predictions_list = predict_all_files(files, 'saved_models/normalized_data/')\n",
    "        per_class_dict = rank_class_performance(predictions_list)\n",
    "        write_to_file(predictions_list, per_class_dict, 'model_testing/new_data/normalized_test.txt')\n",
    "    elif model_type == 2:\n",
    "        predictions_list = predict_all_files(files, 'saved_models/standardized_data/')\n",
    "        per_class_dict = rank_class_performance(predictions_list)\n",
    "        write_to_file(predictions_list, per_class_dict, 'model_testing/new_data/standardized_test.txt')\n",
    "    elif model_type == 3:\n",
    "        predictions_list = predict_all_files(files, 'saved_models/sampling/')\n",
    "        per_class_dict = rank_class_performance(predictions_list)\n",
    "        write_to_file(predictions_list, per_class_dict, 'model_testing/new_data/sampling_test.txt')\n",
    "    else:\n",
    "        print('Non-existent model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fdf984ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 17 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002704CDD3D30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 19 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002704BC92AF0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "# 0 - balancing only\n",
    "# 1 - normalized\n",
    "# 2 - standardized\n",
    "# 3 - sampling\n",
    "if __name__ == \"__main__\":\n",
    "    files = get_files('data\\\\new_data')\n",
    "    perform_test(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c773c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
